{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMTOqdS+dwyawHf5fEDS8xB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"q32meSLO_7N5"},"outputs":[],"source":["import pandas as pd\n","import numpy as np"]},{"cell_type":"code","source":["from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import accuracy_score\n","from scipy.stats import sem\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import make_scorer"],"metadata":{"id":"P7aHDdc5G5WU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def standerdization(blue,red):\n","  Data=(blue-red)/(blue+red)\n","  return Data"],"metadata":{"id":"dt2rzF3ZMA8z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def data(dataframe):\n","  ''' input should be a dataframe\n","      blue dataframe then red dataframe'''\n","standerdization(bluedataframe['blueHeralds'],reddataframe['redHeralds'])\n","standerdization(bluedataframe['blueEliteMonsters'],reddataframe['redEliteMonsters'])\n","drop(dataframe)"],"metadata":{"id":"pXw5AK0pADNX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def drop(dataframe):\n","  dataframe.drop('')"],"metadata":{"id":"Ugz3J-VSTbvJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = GradientBoostingClassifier(n_estimators=10, learning_rate=0.5)"],"metadata":{"id":"UavF6cLm-UF5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prediction(dataframe):\n","  ''' input the train and test data in a dataframe'''\n","\n","def find_optimal_iterations_and_learning_rate(estimator, param_grid, X, y, cv=5, scoring=None):\n","\n","    if scoring is None:\n","        scoring = make_scorer(default_scorer_function)  # Change this to the appropriate scoring function\n","\n","    grid_search = GridSearchCV(estimator, param_grid, cv=cv, scoring=scoring, n_jobs=-1)\n","    grid_search.fit(X, y)\n","\n","    best_params = grid_search.best_params_\n","    return best_params\n","\n","# Usage example\n","\n","param_grid = {\n","    'n_estimators': [30,50,60,70],\n","    'learning_rate': [0.1,0.12,0.14,0.16,0.18],\n","}\n","\n","# Replace X and y with your dataset\n","best_params = find_optimal_iterations_and_learning_rate(\n","    GradientBoostingClassifier(), param_grid, x_train, y_train, cv=5, scoring='accuracy'\n",")\n","\n","print(\"Best Parameters:\", best_params)\n","best_params['learning_rate']\n","best_params['n_estimators']"],"metadata":{"id":"PjNdpMVcH1h3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_cost(X, y, theta):\n","    # Replace this with your actual cost function\n","    predictions = np.dot(X, theta)\n","    cost = np.mean((predictions - y) ** 2)\n","    return cost\n","\n","def gradient_descent(X, y, theta, learning_rate):\n","    # Replace this with your actual gradient descent update\n","    predictions = np.dot(X, theta)\n","    errors = predictions - y\n","    gradient = 2 * np.dot(X.T, errors) / len(y)\n","    theta = theta - learning_rate * gradient\n","    return theta\n","\n","def train_model(X, y, epsilon=1e-6, learning_rate_epsilon=1e-6, learning_rate=0.01, max_iterations=1000):\n","    # Initialize parameters randomly or with zeros\n","    theta = np.zeros(X.shape[1])\n","\n","    # Initial cost\n","    prev_cost = compute_cost(X, y, theta)\n","\n","    # Initial learning rate\n","    prev_learning_rate = learning_rate\n","\n","    # Training loop\n","    iteration = 0\n","    while iteration < max_iterations:\n","        # Update parameters using gradient descent\n","        theta = gradient_descent(X, y, theta, learning_rate)\n","\n","        # Compute the current cost\n","        current_cost = compute_cost(X, y, theta)\n","\n","        # Check for convergence in cost function\n","        if abs(current_cost - prev_cost) < epsilon:\n","            print(f\"Converged after {iteration} iterations.\")\n","            break\n","\n","        # Check for convergence in learning rate\n","        if abs(learning_rate - prev_learning_rate) < learning_rate_epsilon:\n","            print(f\"Learning rate converged after {iteration} iterations.\")\n","            break\n","\n","        # Print the cost every 100 iterations\n","        if iteration % 100 == 0:\n","            print(f\"Iteration {iteration}, Cost: {current_cost}\")\n","\n","        # Update the previous cost and learning rate for the next iteration\n","        prev_cost = current_cost\n","        prev_learning_rate = learning_rate\n","\n","        iteration += 1\n","\n","    return theta\n","\n","\n","# Add a column of ones to X for the bias term\n","X_with_bias = np.c_[np.ones(X.shape[0]), X]\n","\n","# Train the model\n","trained_theta = train_model(X, Y)"],"metadata":{"id":"vkWqWdpfBtxx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = GradientBoostingClassifier(n_estimators=, learning_rate=)"],"metadata":{"id":"2B96bj7O-U2j"},"execution_count":null,"outputs":[]}]}